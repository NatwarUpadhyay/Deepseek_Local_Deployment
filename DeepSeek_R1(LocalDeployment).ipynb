{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeN3lDTAeFFK"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "# Set your Ollama local API endpoint\n",
        "OLLAMA_API_URL = \"http://localhost:11434/api/chat\"\n",
        "\n",
        "def deepseek_query(model_name: str, prompt: str, stream: bool = False) -> str:\n",
        "    \"\"\"\n",
        "    Send a prompt to the specified DeepSeek model via Ollama API and return the model response.\n",
        "\n",
        "    Parameters:\n",
        "      model_name (str): Name of the DeepSeek model to run (e.g., \"deepseek-r1:1.5b\").\n",
        "      prompt (str): The prompt to send to the model.\n",
        "      stream (bool): Whether to enable streaming responses.\n",
        "\n",
        "    Returns:\n",
        "      str: The AI-generated response.\n",
        "    \"\"\"\n",
        "    payload = {\n",
        "        \"model\": model_name,\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "        \"stream\": stream\n",
        "    }\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "\n",
        "    response = requests.post(OLLAMA_API_URL, data=json.dumps(payload), headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        # Extract and return the text from the model's message response.\n",
        "        return result.get(\"message\", {}).get(\"content\", \"No content returned.\")\n",
        "    else:\n",
        "        raise Exception(f\"Error {response.status_code}: {response.text}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage:\n",
        "    model = \"deepseek-r1:1.5b\"  # Change this to your preferred model tag.\n",
        "    user_prompt = \"Explain the advantages of using H800 GPUs for AI model training over H100 GPUs.\"\n",
        "\n",
        "    try:\n",
        "        answer = deepseek_query(model, user_prompt)\n",
        "        print(\"DeepSeek Response:\\n\", answer)\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\", e)"
      ]
    }
  ]
}